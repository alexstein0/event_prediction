{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eda6fd8-6ba1-4da0-9000-421e572c64fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshomes/astein0/.pyenv/versions/long_context/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc187b29e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67df8dc7-969d-4f0e-95e0-b40b7baa290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul 23 15:50:51 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off |   00000000:C1:00.0 Off |                  Off |\n",
      "| 41%   24C    P8             13W /  140W |       1MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caca6234-5f65-49e6-8bc0-4e4e8105ecc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# path = 'openbmb/MiniCPM-2B-dpo-bf16'\n",
    "path = \"openbmb/MiniCPM-2B-128k\"\n",
    "CPM_tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "CPM_model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1224986-3861-41e7-953c-bb75ff99eb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.09s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "af9b81d5-77c1-478d-b1c2-e01e291aa57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input text\n",
    "def create_prompt(pgn_text):\n",
    "    # input_text = f\"I am going to give you a PGN that depicts a chess game and I am going \\\n",
    "    # to ask you questions about the state of the game at different times.\\n\\\n",
    "    # As a reminder, a PGN is a chess game format that contains header information then the game moves as the last line.\\n\\\n",
    "    # As a test run, lets start by only asking one question:\\nWho won the game?\\n\\\n",
    "    # Just output the single word corresponding to the result of the game.\\\n",
    "    # For example, if white won the game output 'white', if black won the game output 'black' and if its a tie output 'tie'.\\n\\\n",
    "    # Here is the PGN:\\n{pgn_text}\"\n",
    "    input_text = f\"{pgn_text}\\nWhat was the result of the game?\\n\\\n",
    "    Your entire response should only be the result of the game (and no other characters or response words):\\\n",
    "    For example, if white won the game, the entire response should be '1-0', if black won the game the response should be '0-1' and if its a tie the response should be '1/2-1/2'.\\n\\\n",
    "    \"\n",
    "\n",
    "    # inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    return input_text\n",
    "\n",
    "assistent_response = \"The result of the game is: \"\n",
    "\n",
    "# prompt = create_prompt(all_pgns[0][\"text\"])\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "242cbf73-9c60-457a-8004-d56266848000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(pgn_text):\n",
    "    input_text = f\"{pgn_text}\\nHow many pieces are on the queenside of the board after 10 full moves (10 for white and 10 for black)?\\n\\\n",
    "    Respond only with an integer representing your answer.\"\n",
    "\n",
    "    return input_text\n",
    "    \n",
    "assistent_response = \"After 10 moves the number of pieces on the queen side of the board is: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bafcf802-8d91-488d-8bd8-6f9c3f4fe372",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a chess bot charged with helping answer questions about chess games.  \\\n",
    "Users are going to give you a PGN that depicts a chess game and then \\\n",
    "ask you questions about the game. As a reminder, a PGN is a chess game format that \\\n",
    "contains header information then the game moves as the last line.\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "db2c3da1-e330-456a-8518-371e0eb466f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_message(system_prompt, user_prompt, assistent_response):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'user', 'content': user_prompt},\n",
    "        {'role': 'assistant', 'content': assistent_response},\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495f27c-0441-4365-821f-9d3ea8f63407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.chat_template\n",
    "# decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "mod = CPM_model\n",
    "tok = CPM_tokenizer\n",
    "\n",
    "mod = model\n",
    "tok = tokenizer\n",
    "\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for pgn in all_pgns:\n",
    "    user_prompt = create_prompt(pgn[\"text\"])\n",
    "    messages = create_message(system_prompt, user_prompt, assistent_response)\n",
    "    formatted_text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tok(formatted_text, return_tensors='pt', add_special_tokens=False)\n",
    "    inputs = {key: tensor.to(mod.device) for key, tensor in inputs.items()}\n",
    "    outputs = mod.generate(**inputs, max_new_tokens=64)\n",
    "    decoded = tok.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "    print(tok.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "    # did it get correct winner?\n",
    "    ref = pgn[\"result\"]\n",
    "    dec = decoded.strip().split()[0]\n",
    "    \n",
    "    # did it count number of pieces correct on board after 10 moves?\n",
    "    # ref = pgn[\"queenside_10\"]\n",
    "    # decoded = decoded.strip()\n",
    "    \n",
    "    pgn[\"output\"] = decoded\n",
    "    correct += 1 if dec == ref else 0\n",
    "    total += 1\n",
    "    if total % 100 == 0:\n",
    "        print(f\"total: {total}, correct: {correct}\")\n",
    "print(correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
