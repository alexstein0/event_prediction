name: gpt2

# Model args
context_length: 128

# Train args
batch_size: 32
train_ratio: 0.9
# accepts either 'causal' fo next token prediction or 'masked'
training_objective: causal  

train_args:
  # output_dir: ${hydra:run.dir}/trainer
  # per_device_train_batch_size: 32
  # per_device_eval_batch_size: 32
  # evaluation_strategy: "steps"
  # eval_steps: 5000
  # logging_steps: 5000
  # gradient_accumulation_steps: 8
  # num_train_epochs: 1
  # weight_decay: 0.1
  # warmup_steps: 1000
  # lr_scheduler_type: "cosine"
  # learning_rate: 0.0005
  # save_steps: 5000
  # fp16: true
  # push_to_hub: False
  # remove_unused_columns: False