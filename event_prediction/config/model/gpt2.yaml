name: gpt2

# Model args
context_length: 128

# Train args
batch_size: 32
train_ratio: 0.9
training_objective: causal # accepts either 'causal' or 'masked'
loss_fn: CrossEntropyLoss # accepts CrossEntropyLoss
optim: AdamW # accepts AdamW
lr: 5e-4
epochs: 2
chkpt_path: ${hydra:run.dir}/chkpt

train_args:
  # output_dir: ${hydra:run.dir}/trainer
  # per_device_train_batch_size: 32
  # per_device_eval_batch_size: 32
  # evaluation_strategy: "steps"
  # eval_steps: 5000
  # logging_steps: 5000
  # gradient_accumulation_steps: 8
  # num_train_epochs: 1
  # weight_decay: 0.1
  # warmup_steps: 1000
  # lr_scheduler_type: "cosine"
  # learning_rate: 0.0005
  # save_steps: 5000
  # fp16: true
  # push_to_hub: False
  # remove_unused_columns: False